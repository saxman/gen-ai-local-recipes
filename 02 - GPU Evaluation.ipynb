{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0475860-ff11-4ec8-b0d6-6d7b20f3c36f",
   "metadata": {},
   "source": [
    "This notebook inspects the NVIDIA graphics capabilities of the system and detemines how much memory is available for models. The other notebooks assume that the system has at least 10 GB of GPU memory (vRAM). If your system has more or less that this amount, you can either choose different sized models (e.g. use flan-t5-large instead of flan-t5-xlarge) or adjust model quantization (e.g. use 8-bit model weights).\n",
    "\n",
    "You can determine the vRAM requirements of specific models, with or without varying levels quantization, at:\n",
    "https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35c8f9e-e56d-4b97-b85b-2a2e40de4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c259bc0b-a5cd-4af6-bda2-a72526a91a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available : True\n",
      "CUDA version   : 12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available : {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version   : {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749d7a7-a278-4036-a8c0-269ae41cacc0",
   "metadata": {},
   "source": [
    "https://pypi.org/project/nvidia-ml-py/\n",
    "https://developer.nvidia.com/management-library-nvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ebc9a6-3852-4e15-858f-cf4deb41b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade nvidia-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c6983cf-8ed2-4934-b782-eac3b5cecdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver Version: 550.107.02\n",
      "Device 0 : NVIDIA GeForce RTX 4090\n",
      "Device 1 : NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "print(f\"Driver Version: {pynvml.nvmlSystemGetDriverVersion()}\")\n",
    "deviceCount = pynvml.nvmlDeviceGetCount()\n",
    "for i in range(deviceCount):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    print(f\"Device {i} : {pynvml.nvmlDeviceGetName(handle)}\")\n",
    "\n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6ebff8-fc92-468d-8bff-bd89f4cdd4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver version : 550.107.02\n",
      "device count : 2\n",
      "device 0 : NVIDIA GeForce RTX 4090\n",
      "device 0 : mem total : 24564 MB\n",
      "device 0 : mem used  : 544 MB\n",
      "device 0 : mem free  : 24019 MB\n",
      "device 1 : NVIDIA GeForce RTX 4090\n",
      "device 1 : mem total : 24564 MB\n",
      "device 1 : mem used  : 2420 MB\n",
      "device 1 : mem free  : 22143 MB\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "utils.print_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563d7b7-839f-40e9-92aa-20ee99f75471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
