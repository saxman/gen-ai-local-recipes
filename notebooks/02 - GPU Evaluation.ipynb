{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0475860-ff11-4ec8-b0d6-6d7b20f3c36f",
   "metadata": {},
   "source": [
    "# 02 - GPU Evaluation\n",
    "\n",
    "This notebook inspects the NVIDIA graphics capabilities of the system and detemines how much memory is available for models. The other notebooks assume that the system has at least 10 GB of GPU memory (vRAM). If your system has more or less that this amount, you can either choose different sized models (e.g. use flan-t5-large instead of flan-t5-xlarge) or adjust model quantization (e.g. use 8-bit model weights).\n",
    "\n",
    "You can determine the vRAM requirements of specific models, with or without varying levels quantization, at:\n",
    "https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c8f9e-e56d-4b97-b85b-2a2e40de4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c259bc0b-a5cd-4af6-bda2-a72526a91a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available : {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version   : {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52638da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ebc9a6-3852-4e15-858f-cf4deb41b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade nvidia-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6983cf-8ed2-4934-b782-eac3b5cecdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "print(f\"Driver Version: {pynvml.nvmlSystemGetDriverVersion()}\")\n",
    "deviceCount = pynvml.nvmlDeviceGetCount()\n",
    "for i in range(deviceCount):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    print(f\"Device {i} : {pynvml.nvmlDeviceGetName(handle)}\")\n",
    "\n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ebff8-fc92-468d-8bff-bd89f4cdd4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "utils.print_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563d7b7-839f-40e9-92aa-20ee99f75471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai-local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
