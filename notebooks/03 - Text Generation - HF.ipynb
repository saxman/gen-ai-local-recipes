{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63b14de-21d0-4145-a2da-d2da2af06ff5",
   "metadata": {},
   "source": [
    "# 03.1 - Transformer Models - Query\n",
    "\n",
    "How to load and query a Hugging Face Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a3bee-98cb-4ea5-97eb-d0c5d1008333",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b91134",
   "metadata": {},
   "source": [
    "The [Accelerate](https://huggingface.co/docs/accelerate/) library allows models layers to be distruibuted across hardware (GPU, CPU, disk drive) with PyTorch. It's required for using 'device_map' when loading a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a187278-6cc8-4020-98e4-35f2ad0122f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "\n",
    "model_kwargs = {\"torch_dtype\": \"auto\", \"device_map\": \"auto\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2105e-4b2d-45cc-b2c6-c18fc9bdc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b0149-befe-4567-86a2-719d35c9ee82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "utils.print_model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9f0fd-76e2-45ae-865a-8a262749f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.9,\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9afd0d-7661-498a-bee3-7919598d2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is the capital of France?\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(input_text, **generate_kwargs)\n",
    "\n",
    "response = outputs[0][input_text.shape[-1] :]\n",
    "\n",
    "print(tokenizer.decode(response, skip_special_tokens=True) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b3b74-8cf3-4b16-bbd4-b697e4179134",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Provide a concise summary of the provided text.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "There once was a man from Nantucket,\n",
    "who wore on his head a large bucket.\n",
    "He was thinking one day,\n",
    "that there's no rain today,\n",
    "so he took his bucket then chucked it.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(input_text, **generate_kwargs)\n",
    "\n",
    "response = outputs[0][input_text.shape[-1] :]\n",
    "\n",
    "print(tokenizer.decode(response, skip_special_tokens=True) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658f49d-73ff-44ca-8b4a-64336fea9fd1",
   "metadata": {},
   "source": [
    "## Using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f442068-2257-498a-95d4-5504cd6d3fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model_id,\n",
    "    **model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048264a6-5479-42de-a6de-ebbfaa885f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What is the capital of France?\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "response = pipe(messages, max_new_tokens=512)\n",
    "messages = response[0][\"generated_text\"]\n",
    "\n",
    "print(messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6538b4f-f542-4890-8730-6fc88a44cfd6",
   "metadata": {},
   "source": [
    "### Streaming output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f1e2b-12ee-45ed-af1e-2d6b571cef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b5d24a-171a-4caf-b90b-1d6285a844e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Recite the Gettysburg Address\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "response = pipe(messages, max_new_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482857e6-ea52-43be-a0c2-33abdfb5918c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
